{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c3a5ce-6918-49e3-b1f1-476d19f3c717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the data \n",
    "# 3. Read the json into the environment\n",
    "\n",
    "df = spark.read.option(\"multiline\",True).json(\"/Volumes/workspace/default/project_shipment_tracking/Swift Assignment 4 - Dataset.json\")\n",
    "\n",
    "# display dataframe\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2888f8c3-a7fe-4654-a5ce-be4e23bcbcce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# perform the initial exploration\n",
    "# check the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed4d72d0-6f92-411b-90d9-dd940799f025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display all column names\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a9d6d87-c7dd-41f0-8699-6aa931fc085d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# count rows and columns\n",
    "print(\"Number of rows:\", df.count())\n",
    "print(\"Number of columns:\", len(df.columns))\n",
    "print(\"Columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa1ff2b1-4657-4dde-bf63-c569bc753d27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check for missing or null values and display the result\n",
    "from pyspark.sql.functions import col,sum,when\n",
    "df.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f288f177-2295-431d-9d60-0e266e7135cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Flatten the Data\n",
    "1. Flatten the data per item array as needed\n",
    "2. Properties needed\n",
    "- Tracking number\n",
    "- Payment type (Prepaid/COD)\n",
    "- Pickup Date Time\n",
    "- Delivery Date Time\n",
    "- Out for Delivery datetime(s)\n",
    "- Shipment weight\n",
    "- Pickup Pincode, City, State\n",
    "- Drop Pincode, City, State\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3724ebd-670b-4a6e-aa44-5915ec13686d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, expr\n",
    "\n",
    "df_exploded = df.select(explode(\"trackDetails\").alias(\"details\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af7d1cbf-cbba-469e-8a64-7cce54ae377a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761806382382}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flat = df_exploded.select(\n",
    "    col(\"details.trackingNumber\").alias(\"TrackingNumber\"),\n",
    "    col(\"details.specialHandlings\")[0][\"paymentType\"].alias(\"PaymentType\"),\n",
    "    \n",
    "    # Pickup details\n",
    "    expr(\"filter(details.datesOrTimes, x -> x.type = 'ACTUAL_PICKUP')[0].dateOrTimestamp\").alias(\"PickupDateTime\"),\n",
    "    expr(\"filter(details.datesOrTimes, x -> x.type = 'ACTUAL_DELIVERY')[0].dateOrTimestamp\").alias(\"DeliveryDateTime\"),\n",
    "    expr(\"transform(filter(details.datesOrTimes, x -> x.type = 'OUT_FOR_DELIVERY'), x -> x.dateOrTimestamp)\").alias(\"OutForDeliveryDateTimes\"),\n",
    "    col(\"details.shipmentWeight.value\").alias(\"ShipmentWeight\"),\n",
    "    \n",
    " \n",
    "    \n",
    "    # Pickup address (from shipperAddress)\n",
    "    col(\"details.shipperAddress.city\").alias(\"PickupCity\"),\n",
    "    col(\"details.shipperAddress.stateOrProvinceCode\").alias(\"PickupState\"),\n",
    "    expr(\"details.events[0].address.postalCode\").alias(\"PickupPincode\"),\n",
    "    \n",
    "    # Drop address (from destinationAddress)\n",
    "    col(\"details.destinationAddress.city\").alias(\"DropCity\"),\n",
    "    col(\"details.destinationAddress.stateOrProvinceCode\").alias(\"DropState\"),\n",
    "    expr(\"details.events[size(details.events)-1].address.postalCode\").alias(\"DropPincode\")\n",
    ")\n",
    "\n",
    "df_flat.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e8da786-6293-4ce8-98a7-539eb0facf2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Transform the data Compute the following inferred field for every shipment: \n",
    "- All date times readable format in IST \n",
    "- Days taken for journey completion (Pickup to Delivery, in number of days) \n",
    "- Number of delivery attempts (Number of times it has been Out for Delivery + Delivered; handle special case where Out For Delivery and Delivered happens on same day)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e30823-94ed-45ef-90ff-38cf03fc6d83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, from_utc_timestamp, datediff, size, expr, array_distinct\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# All date times readable format in IST \n",
    "# Step 1: Convert pickup, delivery, and out-for-delivery datetimes to IST\n",
    "df_transformed = (\n",
    "    df_flat\n",
    "    .withColumn(\"PickupDateTime_IST\", from_utc_timestamp(to_timestamp(col(\"PickupDateTime\")), \"Asia/Kolkata\"))\n",
    "    .withColumn(\"DeliveryDateTime_IST\", from_utc_timestamp(to_timestamp(col(\"DeliveryDateTime\")), \"Asia/Kolkata\"))\n",
    "    .withColumn(\n",
    "        \"OutForDelivery_IST\",\n",
    "        F.transform(col(\"OutForDeliveryDateTimes\"), \n",
    "                    lambda x: from_utc_timestamp(to_timestamp(x), \"Asia/Kolkata\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Days taken for journey completion (Pickup to Delivery, in number of days) \n",
    "# Step 2: Compute days taken for journey (Delivery - Pickup)\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"DaysTakenForJourney\",\n",
    "    datediff(col(\"DeliveryDateTime_IST\"), col(\"PickupDateTime_IST\"))\n",
    ")\n",
    "\n",
    "# Step 3: Compute Number of delivery attempts (Number of times it has been Out for Delivery + Delivered; handle special case where Out For Delivery and Delivered happens on same day)\n",
    "\n",
    "# Remove duplicate same-day attempts (Out for Delivery + Delivered same day)\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"UniqueDeliveryDays\",\n",
    "    array_distinct(F.transform(col(\"OutForDelivery_IST\"), lambda x: F.to_date(x)))\n",
    ")\n",
    "\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"DeliveryAttempts\",\n",
    "    F.when(\n",
    "        col(\"DeliveryDateTime_IST\").isNotNull(),\n",
    "        F.size(col(\"UniqueDeliveryDays\")) + 1  # Add 1 for the final delivery attempt\n",
    "    ).otherwise(F.size(col(\"UniqueDeliveryDays\")))\n",
    ")\n",
    "\n",
    "# Final selection\n",
    "df_final = df_transformed.select(\n",
    "    \"TrackingNumber\",\n",
    "    \"PaymentType\",\n",
    "    \"PickupDateTime_IST\",\n",
    "    \"DeliveryDateTime_IST\",\n",
    "    \"OutForDelivery_IST\",\n",
    "    \"DaysTakenForJourney\",\n",
    "    \"DeliveryAttempts\",\n",
    "    \"ShipmentWeight\",\n",
    "    \"PickupPincode\", \"PickupCity\", \"PickupState\",\n",
    "    \"DropPincode\", \"DropCity\", \"DropState\"\n",
    ")\n",
    "\n",
    "df_final.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "293f3fbe-271a-44ac-a047-4d71df36d9f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Output as a CSV\n",
    "Write out a csv with the following headers\n",
    "- Tracking number\n",
    "- Payment type (Prepaid/COD)\n",
    "- Pickup Date Time in IST\n",
    "- Delivery Date Time in IST\n",
    "- Days taken for delivery\n",
    "- Shipment weight\n",
    "- Pickup Pincode, City, State\n",
    "- Drop Pincode, City, State\n",
    "- Number of delivery attempts needed\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d9f10f-1d60-4450-99ab-f208b5d05fc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write final DataFrame to CSV\n",
    "output_path = \"/Volumes/workspace/default/shipmenttarget/final_shipment_output_csv\"\n",
    "\n",
    "df_final.select(\n",
    "    col(\"TrackingNumber\").alias(\"Tracking number\"),\n",
    "    col(\"PaymentType\").alias(\"Payment type (Prepaid/COD)\"),\n",
    "    col(\"PickupDateTime_IST\").alias(\"Pickup Date Time in IST\"),\n",
    "    col(\"DeliveryDateTime_IST\").alias(\"Delivery Date Time in IST\"),\n",
    "    col(\"DaysTakenForJourney\").alias(\"Days taken for delivery\"),\n",
    "    col(\"ShipmentWeight\").alias(\"Shipment weight\"),\n",
    "    col(\"PickupPincode\").alias(\"Pickup Pincode\"),\n",
    "    col(\"PickupCity\").alias(\"Pickup City\"),\n",
    "    col(\"PickupState\").alias(\"Pickup State\"),\n",
    "    col(\"DropPincode\").alias(\"Drop Pincode\"),\n",
    "    col(\"DropCity\").alias(\"Drop City\"),\n",
    "    col(\"DropState\").alias(\"Drop State\"),\n",
    "    col(\"DeliveryAttempts\").alias(\"Number of delivery attempts needed\")\n",
    ").coalesce(1) \\\n",
    " .write \\\n",
    " .option(\"header\", True) \\\n",
    " .mode(\"overwrite\") \\\n",
    " .csv(output_path)\n",
    "\n",
    "df_final.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24f46eca-8b2f-4df0-ad0d-dcdd39e9fc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Output Summary Statistics as a CSV\n",
    "Output a summary having:\n",
    "- Mean/Median/Mode of days taken for delivery\n",
    "- Mean/Median/Mode of delivery attemps\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff02e81c-9b2d-40d3-8dd8-ede13b80e6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Step 1: Compute mean and median using built-in functions\n",
    "summary_df = df_final.select(\n",
    "    F.mean(\"DaysTakenForJourney\").alias(\"Mean_DaysTakenForDelivery\"),\n",
    "    F.expr(\"percentile(DaysTakenForJourney, 0.5)\").alias(\"Median_DaysTakenForDelivery\"),\n",
    "    F.mean(\"DeliveryAttempts\").alias(\"Mean_DeliveryAttempts\"),\n",
    "    F.expr(\"percentile(DeliveryAttempts, 0.5)\").alias(\"Median_DeliveryAttempts\")\n",
    ")\n",
    "\n",
    "# Step 2: Compute mode for both columns\n",
    "# Mode for DaysTakenForJourney\n",
    "days_mode_df = (\n",
    "    df_final.groupBy(\"DaysTakenForJourney\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "    .limit(1)\n",
    "    .withColumnRenamed(\"DaysTakenForJourney\", \"Mode_DaysTakenForDelivery\")\n",
    "    .select(\"Mode_DaysTakenForDelivery\")\n",
    ")\n",
    "\n",
    "# Mode for DeliveryAttempts\n",
    "attempts_mode_df = (\n",
    "    df_final.groupBy(\"DeliveryAttempts\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "    .limit(1)\n",
    "    .withColumnRenamed(\"DeliveryAttempts\", \"Mode_DeliveryAttempts\")\n",
    "    .select(\"Mode_DeliveryAttempts\")\n",
    ")\n",
    "\n",
    "# Step 3: Combine everything into one summary DataFrame\n",
    "summary_final = summary_df.crossJoin(days_mode_df).crossJoin(attempts_mode_df)\n",
    "\n",
    "# Step 4: Write summary to CSV\n",
    "output_summary_path = \"/Volumes/workspace/default/shipmentsummarystats/shipment_summary_stats_csv\"\n",
    "\n",
    "summary_final.coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(output_summary_path)\n",
    "\n",
    "summary_final.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a9fd61-6ff0-40ca-b78a-5e71d88997aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Shipment_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
